{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmH9qq4_UCDE",
    "outputId": "24b92884-3306-47c6-c884-3a83fa2b543e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Obtaining dependency information for simpletransformers from https://files.pythonhosted.org/packages/16/c8/20d7eede93e320c0746c01d2205bdbeb388c236247244b9428e743a96de9/simpletransformers-0.64.3-py3-none-any.whl.metadata\n",
      "  Downloading simpletransformers-0.64.3-py3-none-any.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.3/42.3 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (4.65.0)\n",
      "Requirement already satisfied: regex in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (2022.7.9)\n",
      "Collecting transformers>=4.31.0 (from simpletransformers)\n",
      "  Obtaining dependency information for transformers>=4.31.0 from https://files.pythonhosted.org/packages/98/46/f6a79f944d5c7763a9bc13b2aa6ac72daf43a6551f5fb03bccf0a9c2fec1/transformers-4.33.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 119.9/119.9 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting datasets (from simpletransformers)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (1.3.0)\n",
      "Collecting seqeval (from simpletransformers)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorboard (from simpletransformers)\n",
      "  Obtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from simpletransformers) (1.5.3)\n",
      "Collecting tokenizers (from simpletransformers)\n",
      "  Obtaining dependency information for tokenizers from https://files.pythonhosted.org/packages/58/82/48a40cfa8e7c0f2bdfa7b9655439ce878782f1173504ca819b24ab95267d/tokenizers-0.14.0-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.14.0-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting wandb>=0.10.32 (from simpletransformers)\n",
      "  Obtaining dependency information for wandb>=0.10.32 from https://files.pythonhosted.org/packages/8a/ab/3b6cce52474f273522b4381f4ed93120f1a196d09a8ba65e1f4615fbaa39/wandb-0.15.11-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.15.11-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting streamlit (from simpletransformers)\n",
      "  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/d6/1f/d3b33ca37a147a428581ec8b4834e63cb6f3e7116acf4e2e10f851f45a97/streamlit-1.27.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading streamlit-1.27.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting sentencepiece (from simpletransformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     -------- ----------------------------- 225.3/977.5 kB 4.7 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 481.3/977.5 kB 6.0 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 809.0/977.5 kB 6.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 977.5/977.5 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers>=4.31.0->simpletransformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (6.0)\n",
      "Collecting tokenizers (from simpletransformers)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/3.5 MB 8.6 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.5/3.5 MB 6.7 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.8/3.5 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 1.2/3.5 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.4/3.5 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.7/3.5 MB 6.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.1/3.5 MB 6.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.4/3.5 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.8/3.5 MB 6.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.1/3.5 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.4/3.5 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.31.0->simpletransformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/f3/c9/6f14c11265ecc9ff06fddae423eee40061c3841a1e44ca1e7b1bb8ce3b48/safetensors-0.3.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.0.4)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/8a/7e/20f7e45878b5aed34320fbeeae8f78acc806e7bd708d00b1c6e64b016f5b/GitPython-3.1.37-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.37-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/62/3a/765a7699a26884dcbf8b071dbe2a2486cc1cafcfb5f5d2e64ffe745dd0c6/sentry_sdk-1.31.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setproctitle (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading setproctitle-1.3.2-cp311-cp311-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,<5,>=3.19.0 from https://files.pythonhosted.org/packages/5e/46/5b9674a33cbf690ffdd79ab1863767a66461cd06ea7aeb9f90e4e50be7a5/protobuf-4.24.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.24.3-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2023.7.22)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.3.6)\n",
      "Collecting xxhash (from datasets->simpletransformers)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/46/14/0302669d5d983ce23dc3870f4f2b16ab1d757a1d7e54a5cfe7a5df37f8e2/xxhash-3.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->simpletransformers)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit->simpletransformers)\n",
      "  Obtaining dependency information for altair<6,>=4.0 from https://files.pythonhosted.org/packages/f2/b4/02a0221bd1da91f6e6acdf0525528db24b4b326a670a9048da474dfe0667/altair-5.1.1-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit->simpletransformers)\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit->simpletransformers)\n",
      "  Obtaining dependency information for cachetools<6,>=4.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.0.0)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (9.4.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit->simpletransformers)\n",
      "  Obtaining dependency information for rich<14,>=10.14.0 from https://files.pythonhosted.org/packages/c1/d1/23ba6235ed82883bb416f57179d1db2c05f3fb8e5d83c18660f9ab6f09c9/rich-13.5.3-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.1.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.7.1)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit->simpletransformers)\n",
      "  Obtaining dependency information for tzlocal<6,>=1.1 from https://files.pythonhosted.org/packages/84/d2/730a87f0dbf184760394a85088d0d2366a5a8a32bc32ffd869a83f1de854/tzlocal-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit->simpletransformers)\n",
      "  Obtaining dependency information for validators<1,>=0.2 from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "     ---------------------------------------- 0.0/4.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.3/4.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.6/4.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.9/4.8 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.2/4.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.5/4.8 MB 6.5 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.8/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 2.1/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 2.4/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.8/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 3.1/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.4/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.7/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 4.1/4.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 4.4/4.8 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.7/4.8 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.8/4.8 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.3.2)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.1.6)\n",
      "Collecting absl-py>=0.4 (from tensorboard->simpletransformers)\n",
      "  Obtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->simpletransformers)\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/75/c5/fb3ed7495c73c0de58b08376a468a35bdb61b89ddfbdb96a37bceb54f959/grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard->simpletransformers)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/d7/88/1826b0c047c48763b36ed854a984127b430a16b70003155d7b19975f1d59/google_auth-2.23.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->simpletransformers)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->simpletransformers)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 0.0/62.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.7/62.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->simpletransformers)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.15.1)\n",
      "Collecting tzdata (from tzlocal<6,>=1.1->streamlit->simpletransformers)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ---------------------------------------- 0.0/341.8 kB ? eta -:--:--\n",
      "     -------------------------------------  337.9/341.8 kB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 341.8/341.8 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.1)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets->simpletransformers)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading simpletransformers-0.64.3-py3-none-any.whl (250 kB)\n",
      "   ---------------------------------------- 0.0/250.8 kB ? eta -:--:--\n",
      "   ---------------------------------------  245.8/250.8 kB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 250.8/250.8 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.6 MB 6.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/7.6 MB 6.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.9/7.6 MB 6.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/7.6 MB 6.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.6/7.6 MB 6.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.9/7.6 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.5/7.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.8/7.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.2/7.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.5/7.6 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.9/7.6 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.2/7.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.5/7.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.8/7.6 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.2/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.5/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.1/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.5/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.8/7.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.0/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.3/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/2.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "   ---------------------------------------- 0.0/519.6 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 256.0/519.6 kB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 519.6/519.6 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading streamlit-1.27.1-py2.py3-none-any.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.5 MB 6.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/7.5 MB 6.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.3/7.5 MB 7.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.6/7.5 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.9/7.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.3/7.5 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.6/7.5 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.9/7.5 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.2/7.5 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.6/7.5 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.8/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.1/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.5/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.8/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.1/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.5/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.1/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.4/7.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.6/7.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.9/7.5 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.2/7.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.5/7.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/5.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.6/5.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.9/5.5 MB 6.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.6/5.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.9/5.5 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.2/5.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.5/5.5 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.5 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.6/5.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.0/5.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.5 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.3/5.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.2/130.2 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading altair-5.1.1-py3-none-any.whl (520 kB)\n",
      "   ---------------------------------------- 0.0/520.6 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 256.0/520.6 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  512.0/520.6 kB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 520.6/520.6 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
      "   ---------------------------------------- 0.0/190.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 190.0/190.0 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.59.0-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.7 MB 7.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.6/3.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.9/3.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.2/3.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.6/3.7 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.9/3.7 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.2/3.7 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.5/3.7 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.8/3.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.1/3.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.4/3.7 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 295.0/295.0 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.24.3-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 0.0/430.5 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 337.9/430.5 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 430.5/430.5 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading rich-13.5.3-py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 239.8/239.8 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.3.3-cp311-cp311-win_amd64.whl (266 kB)\n",
      "   ---------------------------------------- 0.0/266.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 266.1/266.1 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
      "   ---------------------------------------- 0.0/224.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 224.8/224.8 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.4/135.4 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.3.0-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: seqeval, pathtools\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16189 sha256=13892dec86288e444346611f526e1a1ba772fa09b2124f6d11a83651e350fca4\n",
      "  Stored in directory: c:\\users\\mohammed\\appdata\\local\\pip\\cache\\wheels\\bc\\92\\f0\\243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8801 sha256=1a5abaf36ba9602ee1005465a8dcce26884adaab3babad50e22092d8cb9fa282\n",
      "  Stored in directory: c:\\users\\mohammed\\appdata\\local\\pip\\cache\\wheels\\ea\\b7\\8b\\84e94095ea418b9442f5abeba4ca7b0ad52d3fe7b69d6238a6\n",
      "Successfully built seqeval pathtools\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, xxhash, validators, tzdata, tensorboard-data-server, smmap, setproctitle, sentry-sdk, rsa, protobuf, oauthlib, grpcio, docker-pycreds, dill, cachetools, blinker, absl-py, tzlocal, rich, requests-oauthlib, pydeck, multiprocess, huggingface-hub, google-auth, gitdb, transformers, seqeval, google-auth-oauthlib, GitPython, altair, wandb, tensorboard, streamlit, datasets, simpletransformers\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.1.1\n",
      "    Uninstalling transformers-2.1.1:\n",
      "      Successfully uninstalled transformers-2.1.1\n",
      "Successfully installed GitPython-3.1.37 absl-py-2.0.0 altair-5.1.1 blinker-1.6.2 cachetools-5.3.1 datasets-2.14.5 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.10 google-auth-2.23.2 google-auth-oauthlib-1.0.0 grpcio-1.59.0 huggingface-hub-0.17.3 multiprocess-0.70.15 oauthlib-3.2.2 pathtools-0.1.2 protobuf-4.24.3 pydeck-0.8.1b0 requests-oauthlib-1.3.1 rich-13.5.3 rsa-4.9 safetensors-0.3.3 sentencepiece-0.1.99 sentry-sdk-1.31.0 seqeval-1.2.2 setproctitle-1.3.2 simpletransformers-0.64.3 smmap-5.0.1 streamlit-1.27.1 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tokenizers-0.13.3 transformers-4.33.3 tzdata-2023.3 tzlocal-5.0.1 validators-0.22.0 wandb-0.15.11 xxhash-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PqF3Vfa_Tya5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8feNo6wpTya7"
   },
   "outputs": [],
   "source": [
    "OLIDv1_train_df = (pd.read_csv(\"Datasets/olid-train-small.csv\")).drop(\"id\",axis = 1)  # length 5852\n",
    "hasoc_train_df = (pd.read_csv(\"Datasets/hasoc-train.csv\")).drop(\"id\",axis = 1)       # length 5852\n",
    "OLIDv1_test_df = (pd.read_csv(\"Datasets/olid-test.csv\")).drop(\"id\",axis = 1)         # length 860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-67UkmbeEyD",
    "outputId": "3363e338-8e7e-4d7a-cbb0-a522bc9e153e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "\n",
    "# Preprocess and tokenize the text data in the test set\n",
    "def preprocess_text(text):\n",
    "    # Add special tokens [CLS] and [SEP] and tokenize\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=128, pad_to_max_length=True)\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    segment_ids = tokens['token_type_ids']  # BERT doesn't use segment_ids, but it's required for the function call\n",
    "\n",
    "    return input_ids, attention_mask, segment_ids\n",
    "\n",
    "# Apply preprocessing to each row in the test dataframe\n",
    "OLIDv1_test_df['input_ids'], OLIDv1_test_df['attention_mask'], OLIDv1_test_df['segment_ids'] = zip(*OLIDv1_test_df['text'].apply(preprocess_text))\n",
    "OLIDv1_train_df['input_ids'], OLIDv1_train_df['attention_mask'], OLIDv1_train_df['segment_ids'] = zip(*OLIDv1_train_df['text'].apply(preprocess_text))\n",
    "hasoc_train_df['input_ids'], hasoc_train_df['attention_mask'], hasoc_train_df['segment_ids'] = zip(*hasoc_train_df['text'].apply(preprocess_text))\n",
    "\n",
    "OLIDv1_test_df = OLIDv1_test_df.drop('text',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hmLBkKFiTya7"
   },
   "outputs": [],
   "source": [
    "# Split the OLIDv1_train_df into input features and labels\n",
    "X_olid = OLIDv1_train_df[['input_ids', 'attention_mask', 'segment_ids']]\n",
    "Y_olid = OLIDv1_train_df['labels']\n",
    "\n",
    "# Split the hasoc_train_df into input features and labels\n",
    "X_hasoc = hasoc_train_df[['input_ids', 'attention_mask', 'segment_ids']]\n",
    "Y_hasoc = hasoc_train_df['labels']\n",
    "\n",
    "# Split the OLIDv1_train_df dataset into a training set and a validation set\n",
    "X_train_olid, X_val_olid, Y_train_olid, Y_val_olid = train_test_split(X_olid, Y_olid, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_olid = pd.concat([X_train_olid, Y_train_olid], axis=1)\n",
    "val_olid = pd.concat([X_val_olid, Y_val_olid], axis=1)\n",
    "\n",
    "# Split the hasoc_train_df dataset into a training set and a validation set\n",
    "X_train_hasoc, X_val_hasoc, Y_train_hasoc, Y_val_hasoc = train_test_split(X_hasoc, Y_hasoc, test_size=0.2, random_state=42)\n",
    "\n",
    "train_hasoc = pd.concat([X_train_hasoc, Y_train_hasoc], axis=1)\n",
    "val_hasoc = pd.concat([X_val_hasoc, Y_val_hasoc], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "46wczrTwk1rU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ky1tZrNHk0g0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H0e-s79lhVrZ"
   },
   "outputs": [],
   "source": [
    "def train_model(train_df, val_df):\n",
    "    hatebert = ClassificationModel(\n",
    "        \"bert\",\n",
    "        \"GroNLP/hateBERT\", num_labels=2,\n",
    "        args={\n",
    "            # 'reprocess_input_data': True,\n",
    "            \"learning_rate\": 1e-2,\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"num_train_epochs\": 2,\n",
    "            \"train_batch_size\": 128,\n",
    "            \"eval_batch_size\": 128,\n",
    "            \"evaluate_during_training\": True,\n",
    "            \"save_eval_checkpoints\": True,\n",
    "            \"save_model_every_epoch\": False,\n",
    "            \"use_early_stopping\": True,\n",
    "            \"early_stopping_patience\": 3,\n",
    "            \"early_stopping_delta\": 0.1\n",
    "        },\n",
    "\n",
    "        use_cuda=True\n",
    "    )\n",
    "    hatebert.train_model(train_df, eval_df=val_df)\n",
    "    return hatebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VGQh_mqyfEUt"
   },
   "outputs": [],
   "source": [
    "def plot_losses(training_log):\n",
    "    plt.plot(training_log['train_loss'], label='training Loss')\n",
    "    plt.plot(training_log['eval_loss'], label='validation Loss')\n",
    "    plt.title('Training and validation loss over epochs')\n",
    "    plt.xlabel('Evaluation Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C2sKv-7qhvmN"
   },
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(true_labels, predictions):\n",
    "    class_labels = [\" 0\", \" 1\",]\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(len(class_labels), len(class_labels)))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8mkJL9OLhyVt"
   },
   "outputs": [],
   "source": [
    "def calculate_classifier_report(true_labels, predictions):\n",
    "    return classification_report(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BDbU5QxXl01f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "W7J-IyQzMP8L",
    "outputId": "95dc3306-c353-44e9-d9e7-9a65398fde64"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6c9ae33dff04>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhatebert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_olid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_olid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the training log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs/training_progress_scores.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e459f0bed945>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_df, val_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     hatebert = ClassificationModel(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"GroNLP/hateBERT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         args={\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type, model_name, tokenizer_type, tokenizer_name, num_labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cuda:{cuda_device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    389\u001b[0m                     \u001b[0;34m\"'use_cuda' set to True when cuda is unavailable.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0;34m\" Make sure CUDA is available or set use_cuda=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'use_cuda' set to True when cuda is unavailable. Make sure CUDA is available or set use_cuda=False."
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the model\n",
    "hatebert = train_model(train_olid, val_olid)\n",
    "\n",
    "# Load the training log\n",
    "training_log = pd.read_csv(\"outputs/training_progress_scores.csv\")\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(training_log)\n",
    "\n",
    "\n",
    "# Test the model on test set the model\n",
    "result, model_outputs, wrong_predictions = hatebert.eval_model(OLIDv1_test_df)\n",
    "\n",
    "# Get predictions and true labels\n",
    "predictions = np.argmax(model_outputs, axis=1)\n",
    "true_labels = OLIDv1_test_df['labels']\n",
    "\n",
    "# Calculate confusion matrix and plot it\n",
    "confusion_mat = calculate_confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = calculate_classifier_report(true_labels, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dNfXeRRUm9j"
   },
   "outputs": [],
   "source": [
    "asas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fq-P3FNCUIzi"
   },
   "outputs": [],
   "source": [
    "# Assuming you have already preprocessed and tokenized the test data into 'input_ids', 'attention_mask', and 'segment_ids'\n",
    "\n",
    "# Create a new DataFrame with necessary columns\n",
    "test_data = {\n",
    "    'text': OLIDv1_test_df['text'],\n",
    "    'labels': OLIDv1_test_df['labels'],\n",
    "    'input_ids': OLIDv1_test_df['input_ids'],\n",
    "    'attention_mask': OLIDv1_test_df['attention_mask'],\n",
    "    'segment_ids': OLIDv1_test_df['segment_ids']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Now test_df has the required columns: 'text', 'labels', 'input_ids', 'attention_mask', 'segment_ids'\n",
    "\n",
    "# Test the model on the test set\n",
    "result, model_outputs, wrong_predictions = hatebert.eval_model(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6mZZ8g0WaMT"
   },
   "outputs": [],
   "source": [
    "test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISI94RMgWkSb"
   },
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = hatebert.eval_model(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpezK4FbP_Ge"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load test data (assuming you have a test DataFrame called test_olid_df)\n",
    "test_inputs, test_labels = preprocess_test_data(test_olid_df)\n",
    "\n",
    "# Evaluate the model on the preprocessed test data\n",
    "predictions, model_outputs, wrong_predictions = hatebert.eval_model(test_dataloader)\n",
    "\n",
    "\n",
    "# Assuming model_outputs contains the predicted probabilities and test_labels contains the true labels\n",
    "# predictions, model_outputs, wrong_predictions = hatebert.eval_model(test_olid_df)\n",
    "\n",
    "# Get the predicted labels (assuming it's binary classification)\n",
    "predicted_labels = [1 if prob[1] > prob[0] else 0 for prob in model_outputs]\n",
    "\n",
    "# Get the true labels from your test data (assuming it's binary classification)\n",
    "true_labels = test_olid_df['labels'].tolist()  # Replace 'true_labels' with the actual column name in your DataFrame\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czIgr7prQJw5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the labels for the matrix\n",
    "labels = ['Negative', 'Positive']  # Assuming your classes are 'Negative' and 'Positive'\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61JJNy_mZE2s"
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "train_hasoc_df = pd.DataFrame({'text': X_train_hasoc, 'labels': Y_train_hasoc})\n",
    "val_hasoc_df = pd.DataFrame({'text': X_val_hasoc, 'labels': Y_val_hasoc})\n",
    "\n",
    "test_olid_df = OLIDv1_test_df.drop('id', axis=1)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "hatebert = train_model(train_hasoc_df, val_hasoc_df)\n",
    "\n",
    "# Load the training log\n",
    "training_log = pd.read_csv(\"outputs/training_progress_scores.csv\")\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(training_log)\n",
    "\n",
    "# Test the model on test set the model\n",
    "result, model_outputs, wrong_predictions = hatebert.eval_model(test_olid_df)\n",
    "\n",
    "# Get predictions and true labels\n",
    "predictions = np.argmax(model_outputs, axis=1)\n",
    "true_labels = test_olid_df['labels']\n",
    "\n",
    "# Calculate confusion matrix and plot it\n",
    "confusion_mat = calculate_confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = calculate_classifier_report(true_labels, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Izric3c4GISj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
